# report_generator.py
from pathlib import Path

def generate_report(summary: dict, cis: dict, opt: dict, save_path: Path):
    """
    Generate a Markdown report of the A/B test and optimisation results.
    summary: dict with keys 'conv_rate_A', 'conv_rate_B', 'conversions_A', 'conversions_B', etc.
    cis: dict of confidence intervals per campaign
    opt: dict from optimisation containing allocation and expected_conversions
    """

    report = f"""# ğŸ“Š A/B Test Report

## ğŸ” Campaign Conversion Summary
- Campaign A: {summary['conversions_A']} / {summary['n_A']}  
  Conversion Rate = **{summary['conv_rate_A']:.3f}**
- Campaign B: {summary['conversions_B']} / {summary['n_B']}  
  Conversion Rate = **{summary['conv_rate_B']:.3f}**

## ğŸ“‰ Statistical Test (Z-test for proportions)
- Z-score = **{summary['z_score']:.3f}**  
- P-value = **{summary['p_value']:.4f}**

â¡ï¸ Result: {"âœ… Statistically significant difference!" if summary['p_value'] < 0.05 else "âš ï¸ No significant difference detected."}

## ğŸ“Œ Confidence Intervals (95%)
- Campaign A: {cis['A'][0]:.3f} â†’ {cis['A'][1]:.3f}
- Campaign B: {cis['B'][0]:.3f} â†’ {cis['B'][1]:.3f}

## ğŸ’° Optimal Budget Allocation
- Campaign A: {opt['allocation']['A']*100:.1f}%  
- Campaign B: {opt['allocation']['B']*100:.1f}%

ğŸ“ˆ Expected Conversions: **{opt['expected_conversions']:.1f}**

---

## ğŸ“Š Visualisations
- ![Conversion Rates](plots/conversion_rates.png)  
- ![Optimal Budget Allocation](plots/optimal_allocation.png)

---
_Report auto-generated by the A/B Testing Pipeline._
"""

    save_path.write_text(report, encoding="utf-8")
    print(f"ğŸ“ Report saved to {save_path}")
