# report_generator.py
from pathlib import Path

def generate_report(summary: dict, cis: dict, opt: dict, save_path: Path):
    """
    Generate a Markdown report of the A/B test and optimisation results.
    summary: dict with keys 'conv_rate_A', 'conv_rate_B', 'conversions_A', 'conversions_B', etc.
    cis: dict of confidence intervals per campaign
    opt: dict from optimisation containing allocation and expected_conversions
    """

    report = f"""# 📊 A/B Test Report

## 🔎 Campaign Conversion Summary
- Campaign A: {summary['conversions_A']} / {summary['n_A']}  
  Conversion Rate = **{summary['conv_rate_A']:.3f}**
- Campaign B: {summary['conversions_B']} / {summary['n_B']}  
  Conversion Rate = **{summary['conv_rate_B']:.3f}**

## 📉 Statistical Test (Z-test for proportions)
- Z-score = **{summary['z_score']:.3f}**  
- P-value = **{summary['p_value']:.4f}**

➡️ Result: {"✅ Statistically significant difference!" if summary['p_value'] < 0.05 else "⚠️ No significant difference detected."}

## 📌 Confidence Intervals (95%)
- Campaign A: {cis['A'][0]:.3f} → {cis['A'][1]:.3f}
- Campaign B: {cis['B'][0]:.3f} → {cis['B'][1]:.3f}

## 💰 Optimal Budget Allocation
- Campaign A: {opt['allocation']['A']*100:.1f}%  
- Campaign B: {opt['allocation']['B']*100:.1f}%

📈 Expected Conversions: **{opt['expected_conversions']:.1f}**

---

## 📊 Visualisations
- ![Conversion Rates](plots/conversion_rates.png)  
- ![Optimal Budget Allocation](plots/optimal_allocation.png)

---
_Report auto-generated by the A/B Testing Pipeline._
"""

    save_path.write_text(report, encoding="utf-8")
    print(f"📝 Report saved to {save_path}")
